{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gbra8yC3Sh_"
   },
   "source": [
    "# FIT5148 - Big data management and processing\n",
    "\n",
    "# Activity: Parallel Sort and GroupBy#\n",
    "\n",
    "This activity consists of two parts. In the first part, we will learn and build different **serial/parallel sorting algorithms** where the volume of data to be sorted is large and stored in a database. In the second part, we focus on implementing **serial/parallel GroupBy** queries. GroupBy queries involving aggregates are very common in database processing, especially in Online Analytical Processing (OLAP), and data warehouse.\n",
    "\n",
    "This activity will help you to learn how parallel sorting and GroupBy operations can be implemented for parallel database systems.\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Read the code base and comments carefully\n",
    "- After understanding the provided function, run the cell right below it to check if the result is correct.\n",
    "- Read carefully all the **Exercise** tasks below. There are some code blocks that **you need to complete** yourself.\n",
    "\n",
    "**After this assignment you will:**\n",
    "- Be able to build serial/parallel sorting algorithms\n",
    "- Be able to build serial/parallel GroupBy operations\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebmxLGyP3SiB"
   },
   "source": [
    "<font color='blue'>\n",
    "**What you need to remember**:\n",
    "- Run your cells using SHIFT+ENTER (or \"Run cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oceB1CpU3SiC"
   },
   "source": [
    "### Dataset ###\n",
    "In this activity, we use the following dataset R consisting of numbers for simplicity. In the real world, each number indicates a record. R indicates our experimental entire record set that contains unordered numbers ranging from 1 to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjOsOotf3SiC"
   },
   "outputs": [],
   "source": [
    "R = [8, 12, 16, 4, 11, 15, 3, 7, 14, 2, 6, 10, 1, 5, 9, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4K2O3c13SiG"
   },
   "source": [
    "## Quicksort Algorithm ##\n",
    "\n",
    "Throughout this activity, as an internal sorting method, we will use the quicksort method. In internal sorting, sorting takes place totally within main memory. The data to be sorted is assumed to be small and fits into main memory. This sorting method will be commonly used in the serial/parallel external sorting methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75B_Ly0k3SiH"
   },
   "outputs": [],
   "source": [
    "def qsort(arr): \n",
    "\n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        #take the first element as the pivot\n",
    "        pivot = arr[0]\n",
    "        left_arr = [x for x in arr[1:] if x < pivot]\n",
    "        right_arr = [x for x in arr[1:] if x >= pivot]\n",
    "        # uncomment this to see what to print \n",
    "        # print(\"Left:\" + str(left_arr)+\" Pivot : \"+ str(pivot)+\" Right: \" + str(right_arr))\n",
    "        value = qsort(left_arr) + [pivot] + qsort(right_arr)\n",
    "        \n",
    "        return value\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWHznl7_3SiJ",
    "outputId": "f14472ae-aca4-45d0-a824-ddbce36b06fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qsort(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlCk4HDa3SiP"
   },
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "    <tr align=\"left\">\n",
    "     <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpY9rcsQ3SiQ"
   },
   "source": [
    "## 1. Serial External Sorting based on Sort-Merge ##\n",
    "\n",
    "The serial sorting method we consider is **serial external sorting** which is external sorting in a uniprocessor environment. The most common serial external sorting algorithm is based on **sort-merge**. The underlying idea is that we (1) break the given record set into unsorted sub-record sets, (2) sort the sub-record sets, and (3) merge them into larger and larger sorted sub-record sets until the entire record set is sorted. In the real-word, **each sub-record set** is replaced by **a file**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhNVgd453SiR"
   },
   "source": [
    "### Note ###\n",
    "It is important to determine the size of each sub-record set to be sorted. Each sub-record set must be small enough to fit into the main memory. The size of these sub-record sets is determined by the **buffer size in main memory**, which is to be used for sorting each sub-record set internally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6CKkBNYb3SiR"
   },
   "source": [
    "**Exercise**: Understand and run the following serial external sorting algorithm. Then, discuss the time complexity of this algorithm as well as its pros and cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHB0MFiT3SiS"
   },
   "outputs": [],
   "source": [
    "# Let's first look at 'k-way merging algorithm' that will be used \n",
    "# to merge sub-record sets in our external sorting algorithm.\n",
    "import sys\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0]\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if(records[i] < m):  \n",
    "            index = i\n",
    "            m = records[i]\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of mulitple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []  \n",
    "    \n",
    "    while(True):\n",
    "        merged_result = [] # the merging unit (i.e. # of the given buffers)\n",
    "        \n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                merged_result.append(sys.maxsize)\n",
    "            else:\n",
    "                merged_result.append(record_sets[i][indexes[i]])  \n",
    "        \n",
    "        # find the smallest record \n",
    "        smallest = find_min(merged_result)\n",
    "    \n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(merged_result[smallest] == sys.maxsize):\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_z7oirf3SiV",
    "outputId": "5a3ed32e-7057-4cf9-ba83-96b473cfb9f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "# Test k-way merging method\n",
    "buffers = [[1, 2, 3, 4, 8, 13], [5, 6, 7, 11, 12], [9, 10, 14, 15, 16]]\n",
    "result = k_way_merge(buffers)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wk8D2z4j3SiY"
   },
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "    <tr align=\"left\">\n",
    "     <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRuFvkug3SiZ"
   },
   "outputs": [],
   "source": [
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of eac sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # --- Sort Phase ---\n",
    "    sorted_set = []\n",
    "    \n",
    "    # Read buffer_size pages at a time into memory and\n",
    "    # sort them, and write out a sub-record set (i.e. variable: subset)\n",
    "    start_pos = 0\n",
    "    N = len(dataset)\n",
    "    while True:\n",
    "        if ((N - start_pos) > buffer_size):\n",
    "            # read B-records from the input, where B = buffer_size\n",
    "            subset = dataset[start_pos:start_pos + buffer_size] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = qsort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            start_pos += buffer_size\n",
    "        else:\n",
    "            # read the last B-records from the input, where B is less than buffer_size\n",
    "            subset = dataset[start_pos:] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = qsort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            break\n",
    "    \n",
    "    # --- Merge Phase ---\n",
    "    merge_buffer_size = buffer_size - 1\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        while True:\n",
    "            if ((N - start_pos) > merge_buffer_size): \n",
    "                # read C-record sets from the merged record sets, where C = merge_buffer_size\n",
    "                subset = dataset[start_pos:start_pos + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                start_pos += merge_buffer_size\n",
    "            else:\n",
    "                # read C-record sets from the merged sets, where C is less than merge_buffer_size\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                break\n",
    "\n",
    "        dataset = merged_set\n",
    "        if (len(dataset) <= 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N1xUg8-3Sib",
    "outputId": "2bae6555-911c-43b4-aa51-39906a7a1e45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sorting result:[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n"
     ]
    }
   ],
   "source": [
    "result = serial_sorting(R, 4)\n",
    "print(\"final sorting result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP7yPkhi3Sie"
   },
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "    <tr align=\"left\">\n",
    "     <td>final sorting result:[[1, 2, 3, 5, 6, 4, 7, 9, 10, 8, 11, 13, 14, 12, 15, 16]]</td> \n",
    "    </tr>\n",
    " \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNhbAVfR3Sif"
   },
   "source": [
    "## 2.  Algorithms for Parallel External Sort ##\n",
    "Having practiced how serial external sorting works, let's move onto building parallel sorting methods. In the lectures, you have learned a number of different parallel sorting methods. For this activity, we focus on two widely-used parallel external sorting methods: **(1) parallel merge-all sort, and (2) parallel binary-merge sort**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1zkZIfa3Sig"
   },
   "source": [
    "### 2.1 Parallel Merge-All Sort ###\n",
    "The Parallel merge-all sort method is a traditional approach and is composed of two phases: **(1) local sort** and **(2) final merge**. The first phase is carried out independently in each processor. Local sorting in each processor is performed as per a normal serial external sorting mechanism. In the final merge phase, the results from the local sort phase are merged. The final merge phase is carried out by one processor, namely, the host using k-way mergin (see function `k_way_merge()` above)\n",
    "\n",
    "**Exercise**: **Complete the parallel merge-all sort algorithm** by implementing the following code block between '### START CODE HERE ###' and '### END CODE HERE ###'. Assume that we use the serial sorting method defined above (see above **`serial_sorting()`**) and a data partitioning method, round-robin data partitioning method designed for **\"Parallel Search\"** acitivity (i.e. see below: **`rr_partition()`**).  Further, discuss the pros and cons of this algorithm. Also, compare it with the above serial external sorting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQ3P8Qtr3Sih"
   },
   "source": [
    "#### Use the round-robin data partitioning method ####\n",
    "As a pre-requiste process, we first need to partition the given data into a number of subsets according to the number of parallel processors available. As mentioned above, let's assume that we use the round-robin partitioning method. Refer to the \"Parallel Search\" activity and copy the `rr_partition()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5iVIwoK3Sii"
   },
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Calculate the number of the elements to be allocated to each bin\n",
    "    n_bin = len(data)/n\n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (index % n)\n",
    "        result[index_bin].append(element)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BH-lJ6vT3Sil",
    "outputId": "df166834-a156-4e8e-e4ac-d8f7bd0f0692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 11, 14, 1], [12, 15, 2, 5], [16, 3, 6, 9], [4, 7, 10, 13]]\n"
     ]
    }
   ],
   "source": [
    "# Test the round-robin partitioning function\n",
    "result = rr_partition(R, 4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8lV-Dxf3Sio"
   },
   "source": [
    "**Expected Output**: \n",
    "<table align='center'>\n",
    "    <tr>\n",
    "     <td>[[8, 11, 14, 1], [12, 15, 2, 5], [16, 3, 6, 9], [4, 7, 10, 13]]</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8JJbTM33Sip"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_merge_all_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge-all sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Pre-requisite: Perform data partitioning using round-robin partitioning\n",
    "    subsets = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Sort phase -----\n",
    "    sorted_set = []\n",
    "    for s in subsets:\n",
    "        # call the serial_sorting method above\n",
    "        sorted_set.append(*pool.apply_async(serial_sorting, [s, buffer_size]).get())\n",
    "    pool.close()\n",
    "    \n",
    "    # ---- Final merge phase ----\n",
    "    print(\"sorted entire set:\" + str(sorted_set))\n",
    "    result = k_way_merge(sorted_set)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0Kb1mko3Sis",
    "outputId": "dccdf75e-b68c-4f2a-e628-ad2573578c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted entire set:[[1, 8, 11, 14], [2, 5, 12, 15], [3, 6, 9, 16], [4, 7, 10, 13]]\n",
      "final result:[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_merge_all_sorting(R, 4, 4)\n",
    "print(\"final result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzcWwQmV3Siv"
   },
   "source": [
    "**Expected Output**: \n",
    "<table aligh='left'>\n",
    "    <tr><td>sorted entire set:[[1, 8, 11, 14], [2, 5, 12, 15], [3, 6, 9, 16], [4, 7, 10, 13]]</td></tr>\n",
    "    <tr><td>final result:[1, 2, 3, 4, 5, 6, 7, 9, 10, 13, 8, 11, 12, 14, 15, 16]</td>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdTTp0sd3Six"
   },
   "source": [
    "## 2.2 Parallel Binary-Merge Sort ###\n",
    "The Parallel binary-merge sort method also takes two phases as the parallel merge-all sort: **(1) local sort** and **(2) final merge**. The first phase is similar to the parallel merge-all sort. The second phase, the merging phase, is pipelined instead of concentrating on one processor. In this phase, we take the results from two processors and then merging the two in one processor, called **binary merging**. The result of the merging between two processors is passed on to the next level until one processor (the host) is left. \n",
    "\n",
    "**Exercise**: **Complete the parallel merge-all sort algorithm** by implementing the following code block between '### START CODE HERE ###' and '### END CODE HERE ###'. Assume that we use the same partitioning method as the parallel merge-all sort (i.e. **`rr_partition()`**). Further, discuss the pros and cons of this algorithm with comparing with the parallel merge-all sort method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLYjB0lq3Siy"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_binary_merge_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel binary-merge sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Pre-requisite: Perform data partitioning using round-robin partitioning\n",
    "    subsets = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Sort phase -----\n",
    "    sorted_set = []\n",
    "    for s in subsets:\n",
    "        # call the serial_sorting method above\n",
    "        sorted_set.append(*pool.apply_async(serial_sorting, [s, buffer_size]).get())\n",
    "    pool.close()\n",
    "    \n",
    "    # ---- Final merge phase ----\n",
    "    print(\"sorted entire set:\" + str(sorted_set))\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        pool = mp.Pool(processes = N//2)\n",
    "\n",
    "        while True:\n",
    "            if ((N - start_pos) > 2): \n",
    "                subset = dataset[start_pos:start_pos + 2]\n",
    "                merged_set.append(pool.apply(k_way_merge, [subset]))\n",
    "                start_pos += 2\n",
    "            else:\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(pool.apply(k_way_merge, [subset]))\n",
    "                break\n",
    "        \n",
    "        pool.close()\n",
    "        dataset = merged_set\n",
    "        \n",
    "        if (len(dataset) == 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMmEIoUp3Si5",
    "outputId": "ef2a0ed0-5bd6-4958-fb25-03c3cc39ca7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted entire set:[[1, 8, 11, 14], [2, 5, 12, 15], [3, 6, 9, 16], [4, 7, 10, 13]]\n",
      "final result:[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_binary_merge_sorting(R, 4, 3)\n",
    "print(\"final result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O8mUv98c3Si-"
   },
   "source": [
    "**Expected Output**: \n",
    "<table aligh='left'>\n",
    "    <tr><td>sorted entire set:[[1, 8, 11, 14], [2, 5, 12, 15], [3, 6, 9, 16], [4, 7, 10, 13]]\n",
    "</td></tr>\n",
    "    <tr><td>final result:[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]</td>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "JBseXWSU3Si_"
   },
   "source": [
    "## 3. Parallel Algorithms for GroupBy Queries ##\n",
    "\n",
    "Parallel aggregate processing is very similar to parallel sorting. From the lessons we learned from parallel sorting, we focus on one parallel aggregate query algorithms: **A traditional merge-all method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "nkQpP0Ph3SjA"
   },
   "source": [
    "## 3.1 Merge-All GroupBy Method ###\n",
    "This method takes two phases: (1) a local aggregation step, and (2) a global aggregation step.\n",
    "In the first step, each processor groups local records according to the designated group-by attribute and performs the aggregate function.  The second step is a global aggregation step, in which all the temporary results obtained in each node are passed to the host for consolidation in order to produce the global aggregate values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CX5jcCsI3SjB"
   },
   "source": [
    "### The dataset for the GroupBy implementation ###\n",
    "Let's assume that we have two different datasets `D1` and `D2` where each dataset will be handled by a processor in a local aggregation step. In the second global aggregation step, the aggreated results will be handled by the host. Each record is represented by a nominal key and a numeric value. Note that duplicated keys exist in `D1` and `D2`. For our GroupBy implementation, we retrieve pairs of keys and values according to the key attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "INRtBRR63SjC"
   },
   "outputs": [],
   "source": [
    "D1 = [('A', 1), ('B', 2), ('C', 3), ('A', 10), ('B', 20), ('C', 30)]\n",
    "D2 = [('A', 4), ('B', 5), ('C', 6), ('A', 40), ('B', 50), ('C', 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-rOSDDx3SjE"
   },
   "source": [
    "**Exercise**: Understand and run the fist phase of the parallel merge-all GroupBy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQSxz3mM3SjF"
   },
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "def local_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[0]\n",
    "        val = record[1]\n",
    "        if key not in dict:\n",
    "            dict[key] = 0\n",
    "        dict[key] += val\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gITmt_OY3SjL",
    "outputId": "f89560b9-6eda-4626-86e6-21b0cd4c236c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 11, 'B': 22, 'C': 33}\n",
      "{'A': 44, 'B': 55, 'C': 66}\n"
     ]
    }
   ],
   "source": [
    "result = local_groupby (D1)\n",
    "print(result)\n",
    "result = local_groupby (D2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MXGkRUc3SjO"
   },
   "source": [
    "**Expected Output**: \n",
    "<table aligh='left'>\n",
    "    <tr><td>{'A': 11, 'B': 22, 'C': 33}</td></tr>\n",
    "    <tr><td>{'A': 44, 'B': 55, 'C': 66}</td></tr>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdG_s4py3SjP"
   },
   "source": [
    "**Exercise**: **Complete the parallel merge-all groupby algorithm** by implementing the following code block between '### START CODE HERE ###' and '### END CODE HERE ###'. You need to use the local aggregation method defined above (i.e. **local_groupby()**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RR_ahJWP3SjQ"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_merge_all_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(dataset)\n",
    "\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Local aggregation step -----\n",
    "    # Implement here\n",
    "    local_result = []\n",
    "    for s in dataset:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby, [s]))\n",
    "    pool.close()\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    # Let's assume that the global operator is sum.\n",
    "    # Implement here\n",
    "    for r in local_result:\n",
    "        for key, val in r.items():\n",
    "            if key not in result:\n",
    "                result[key] = 0\n",
    "            result[key] += val    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJzIusrL3SjT",
    "outputId": "f4f392ca-5152-454f-bf37-250eb1086014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 55, 'B': 77, 'C': 99}\n"
     ]
    }
   ],
   "source": [
    "E = [D1, D2]\n",
    "result = parallel_merge_all_groupby (E)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTdBKpGJ3SjX"
   },
   "source": [
    "**Expected Output**: \n",
    "<table aligh='left'>\n",
    "    <tr><td>{'A': 55, 'B': 77, 'C': 99}</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EEgkugl3SjY"
   },
   "source": [
    "Congratulations on finishing this activity!\n",
    "\n",
    "<font color='blue'>\n",
    "**Wrap up what we've learned:**\n",
    "- Internal sorting takes place totally within main memory. The data\n",
    "to be sorted is assumed to be small and fits into main memory. External sorting on the other hand is where the volume of data to be sorted is large and resides in secondary memory. Thus external sorting is also known as file sorting.\n",
    "- We practiced that how serial external sorting can be implemented using the k-way merge operation.\n",
    "- We are now able to build parallel external sorting methods using the serial external sorting methods: (1) parallel merge-all sort, and (2) parallel binary-merge sort\n",
    "- We now now able to build a parallel groubby method consisting of two phases: (1) a local aggregation step, and (2) a global aggregation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67qddkUX3SjZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5148 - Parallel_Sort_And_GroupBy_Solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
