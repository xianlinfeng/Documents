{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD5VW0TzHdR_"
   },
   "source": [
    "# FIT5148 - Distributed Databases and Big Data\n",
    "\n",
    "# Take Home Test - Solution Workbook#\n",
    "\n",
    "This test consists of three questions total worth 5% of the final marks. The first question is related to ** Parallel Search Algorithms (1 Marks)**, the second question is related to ** Parallel Join Algorithms (2 Marks)** and the third question is realted to ** Parallel Sort and GroupBy Algorithms (2 Marks)**.\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Read the instructions, code base and comments carefully.\n",
    "- There are code blocks that **you need to complete** yourself as a part of test.\n",
    "- <font color='red'> **Comment each line of code properly such that the tutor can easily understand what you are trying to do in the code.**</font>\n",
    "\n",
    "**Your Details:**\n",
    "- Name: <font color='blue'>  Xianlin Feng </font>\n",
    "- StudentID:  <font color='blue'>  28847458 </font>\n",
    "- Email:  <font color='blue'>  xfen0007@student.monash.edu </font>\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRWxd1YrHdSB"
   },
   "source": [
    "\n",
    "\n",
    "### Dataset ###\n",
    "For this test, we will use the following two tables R and S to write the solutions to three parallel algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImhBOqgmHdSC"
   },
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "R = [('Adele',8),('Bob',22),('Clement',16),('Dave',23),('Ed',11),\n",
    "     ('Fung',25),('Goel',3),('Harry',17),('Irene',14),('Joanna',2),\n",
    "     ('Kelly',6),('Lim',20),('Meng',1),('Noor',5),('Omar',19)]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "S = [('Arts',8),('Business',15),('CompSc',2),('Dance',12),('Engineering',7),\n",
    "     ('Finance',21),('Geology',10),('Health',11),('IT',18)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7W3uUvJLErBY"
   },
   "source": [
    "### 1. Parallel Searching Algorithm ###\n",
    "In this task, you will build a **parallel search algorithm for range selection (continuous)** for a given query. You will implement one particular search algorithm which is instructed below.\n",
    "\n",
    " **Implement a parallel search algorithm** that uses the linear search algorithm (i.e. **`linear_search()`**) and is able to work with the hash partitioning method (i.e.**` h_partition()`**). \n",
    " **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvTYiOkyFJp8"
   },
   "outputs": [],
   "source": [
    "# Linear search function\n",
    "def linear_search(data, key):\n",
    "    \"\"\"\n",
    "    Perform linear search on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list or a numpy array\n",
    "    key -- an query record\n",
    "\n",
    "    Return:\n",
    "    result -- the position of searched record\n",
    "    \"\"\"\n",
    "    \n",
    "    matched_records = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    position = -1\n",
    "    for d in data:\n",
    "        if d[1] == key:\n",
    "            matched_records.append(d)\n",
    "            break\n",
    "         \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return matched_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "An_0xFW2FQvs"
   },
   "outputs": [],
   "source": [
    "# Define a simple hash function.\n",
    "def s_hash(x, n):\n",
    "    \"\"\"\n",
    "    Define a simple hash function for demonstration\n",
    "\n",
    "    Arguments:\n",
    "    x -- an input record\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the hash value of x\n",
    "    \"\"\"\n",
    "    result = x%n \n",
    " \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMLZCK2BFYeF"
   },
   "outputs": [],
   "source": [
    "# Hash data partitionining function. \n",
    "# We will use the \"s_hash\" function defined above to realise this partitioning\n",
    "def h_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform hash data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    partitions = {}\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    for d in data:\n",
    "        h = s_hash(d[1],n)\n",
    "        if (h in partitions.keys()):\n",
    "            s = partitions[h]\n",
    "            s.add(d)\n",
    "            partitions[h] = s\n",
    "        else:\n",
    "            s = set()\n",
    "            s.update({d})\n",
    "            partitions[h] = s\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlVKTCO-FkV9"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# Parallel searching algorithm for range selection\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    \"\"\"\n",
    "    Perform parallel search for range selection on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- the input dataset which is a list\n",
    "    query_range -- a query record in the form of a range (e.g. [30, 50])\n",
    "    n_processor -- the number of parallel processors\n",
    "    \n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    pool = Pool(processes=n_processor)\n",
    "\n",
    "    ### START CODE HERE ###        \n",
    "\n",
    "    # Perform data partitioning first\n",
    "    DD = h_partition(data,n_processor)\n",
    "    for query in range(query_range[0], (query_range[1]+1), 1):\n",
    "        query_hash = s_hash(query, n_processor)\n",
    "        d = list(DD[query_hash])\n",
    "        result = pool.apply(linear_search, [d, query])\n",
    "        results.append(result)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgfN1IcyFxaG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Noor', 5)], [('Kelly', 6)], [], [('Adele', 8)], [], [], [('Ed', 11)], [], [], [('Irene', 14)], [], [('Clement', 16)], [('Harry', 17)], [], [('Omar', 19)], [('Lim', 20)]]\n"
     ]
    }
   ],
   "source": [
    "n_processor = 3\n",
    "# Range partition, linear_search \n",
    "results = parallel_search_range(R, [5, 20], n_processor)\n",
    "# the results I get from this method, both include the lower bound and the upper bound.\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qf29MfmUGWV1"
   },
   "source": [
    "## 2. Parallel Join Algorithm\n",
    "\n",
    "In this task, you will implement a **disjoint-partitioning based parallel join algorithm**. This algorithm consist of two stages: a data partitioning stage using a disjoint partitioning and a local join.\n",
    "\n",
    " \n",
    "As a data partitioning method, use the range partitioninig method  (i.e. **`range_partition( )`**).\n",
    "Assume that we have **3 parallel processors**, processor 1 will get records with join attribute value between 1 and 9, processor 2 between 10 and 19, and processor 3 between 20 and 29. Note that both tables R and S need to be partitioned based on the join attribute with the same range partitioning function. \n",
    "\n",
    "As a joining technique, use the hash based join algorithm (i.e.**`HB_join( )`** ).  **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_BwmzrmHaTN"
   },
   "outputs": [],
   "source": [
    "# Range data partitionining function (Need to modify as instructed above)\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    range_indices -- the index list of ranges to be s:plit\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ###  \n",
    "   # this is for range_partition\n",
    "    new_data = list(data)\n",
    "    new_data.sort(key=lambda ele:ele[1])\n",
    "    n_bin = len(range_indices) \n",
    "    \n",
    "    for i in range(n_bin):\n",
    "        s = [x for x in new_data if x[1] < range_indices[i] ]\n",
    "        result.append(s)\n",
    "        last_element = s[len(s)-1]\n",
    "        last = new_data.index(last_element)\n",
    "        new_data = new_data[int(last)+1:]\n",
    "    result.append([x for x in new_data if x[1] >= range_indices[n_bin-1]]) \n",
    "       \n",
    "  \n",
    "   \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kONrahsIMmD"
   },
   "outputs": [],
   "source": [
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "    \n",
    "    # Calulate the sum of elemenets in the digits\n",
    "    return sum(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEpFbToJIPlr"
   },
   "outputs": [],
   "source": [
    "def HB_join(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \n",
    "    dic = {} # We will use a dictionary\n",
    "    \n",
    "    # For each record in table T2\n",
    "    for s in T2:\n",
    "        # Hash the record based on join attribute value using hash function H into hash table\n",
    "        s_key = H(s)\n",
    "        if s_key in dic:\n",
    "            dic[s_key].add(s) # If there is an entry\n",
    "        else:\n",
    "            dic[s_key] = {s}\n",
    "            \n",
    "    # For each record in table T1 (probing)\n",
    "    for r in T1:\n",
    "        # Hash the record based on join attribute value using H\n",
    "        r_key = H(r)\n",
    "\n",
    "        # If an index entry is found Then\n",
    "        if r_key in dic:\n",
    "            # Compare each record on this index entry with the record of table T1\n",
    "            for item in dic[r_key]:\n",
    "                if item[1] == r[1]:\n",
    "                    # Put the rsult\n",
    "                    result.append({\", \".join([r[0], str(r[1]), item[0]])})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cpQYKvvH241"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def DPBP_join(T1, T2, n_processor):\n",
    "    \"\"\"\n",
    "    Perform a disjoint partitioning-based parallel join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- the number of parallel processors\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Partition T1 & T2 into sub-tables using range_partition().\n",
    "    # The number of the sub-tables must be the equal to the n_processor\n",
    "    T1_subsets = range_partition(T1, [10, 20])\n",
    "    T2_subsets = range_partition(T2, [10, 20])\n",
    "\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    midResults = []\n",
    "    for i in range(len(T1_subsets)):\n",
    "        # Apply a join on each processor\n",
    "        output = pool.apply_async(HB_join, [T1_subsets[i], T2_subsets[i]])\n",
    "        midResults.append(output)\n",
    "        \n",
    "    for result in midResults:\n",
    "        results.append(result.get())\n",
    "    \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJgTe8pVH_0z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Joanna, 2, CompSc'}, {'Adele, 8, Arts'}], [{'Ed, 11, Health'}], []]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_processor = 3\n",
    "DPBP_join(R, S, n_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9vIqdbjJaXv"
   },
   "source": [
    "## 3. Parallel Sorting Algorithm\n",
    "\n",
    "In this task, you will implement **parallel binary-merge sort** method. It has two phases same as the parallel merge-all sort that you learnt in the labs: (1) local sort and (2) final merge. The first phase is similar to the parallel merge-all sort. The second phase, the merging phase, is pipelined instead of concentrating on one processor. In this phase, we take the results from two processors and then merging the two in one processor, called binary merging. The result of the merging between two processors is passed on to the next level until one processor (the host) is left.\n",
    "\n",
    " **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**\n",
    "Assume that we use the round robin partitioning method  (i.e. **`rr_partition()`**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd16AZF_LgWp"
   },
   "outputs": [],
   "source": [
    "# You will have to edit qsort(arr) to make it work.\n",
    "def qsort(arr): \n",
    "\n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        #take the first element as the pivot\n",
    "        pivot = arr[0]\n",
    "        left_arr = [x for x in arr[1:] if x[1] < pivot[1]] # Edit is required here\n",
    "        right_arr = [x for x in arr[1:] if x[1] >= pivot[1]] # Edit is required here\n",
    "        # uncomment this to see what to print \n",
    "        # print(\"Left:\" + str(left_arr)+\" Pivot : \"+ str(pivot)+\" Right: \" + str(right_arr))\n",
    "        value = qsort(left_arr) + [pivot] + qsort(right_arr)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3vxcrs-LVaG"
   },
   "outputs": [],
   "source": [
    "# You will have to edit find_min(records) and k_way_merge(record_sets) to make it work.\n",
    "import sys\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0]\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if(records[i][1] < m[1]):  # Edit is required here\n",
    "            index = i\n",
    "            m = records[i]\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of mulitple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []  \n",
    "    \n",
    "    while(True):\n",
    "        merged_result = [] # the merging unit (i.e. # of the given buffers)\n",
    "        \n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                merged_result.append(('MaxNumber',sys.maxsize)) # Edit is required here\n",
    "            else:\n",
    "                merged_result.append(record_sets[i][indexes[i]])  \n",
    "        \n",
    "        # find the smallest record \n",
    "        smallest = find_min(merged_result)\n",
    "    \n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(merged_result[smallest][1] == sys.maxsize): # Edit is required here\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yof8Q84YLcOU"
   },
   "outputs": [],
   "source": [
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of eac sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # --- Sort Phase ---\n",
    "    sorted_set = []\n",
    "    \n",
    "    # Read buffer_size pages at a time into memory and\n",
    "    # sort them, and write out a sub-record set (i.e. variable: subset)\n",
    "    start_pos = 0\n",
    "    N = len(dataset)\n",
    "    while True:\n",
    "        if ((N - start_pos) > buffer_size):\n",
    "            # read B-records from the input, where B = buffer_size\n",
    "            subset = dataset[start_pos:start_pos + buffer_size] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = qsort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            start_pos += buffer_size\n",
    "        else:\n",
    "            # read the last B-records from the input, where B is less than buffer_size\n",
    "            subset = dataset[start_pos:] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = qsort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            break\n",
    "    \n",
    "    # --- Merge Phase ---\n",
    "    merge_buffer_size = buffer_size - 1\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        while True:\n",
    "            if ((N - start_pos) > merge_buffer_size): \n",
    "                # read C-record sets from the merged record sets, where C = merge_buffer_size\n",
    "                subset = dataset[start_pos:start_pos + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                start_pos += merge_buffer_size\n",
    "            else:\n",
    "                # read C-record sets from the merged sets, where C is less than merge_buffer_size\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                break\n",
    "\n",
    "        dataset = merged_set\n",
    "        if (len(dataset) <= 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Calculate the number of the elements to be allocated to each bin\n",
    "    n_bin = len(data)/n\n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (index % n)\n",
    "        result[index_bin].append(element)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_jH8jXwLKRT"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_binary_merge_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel binary-merge sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    # Pre-requisite: Perform data partitioning using round-robin partitioning\n",
    "    subsets = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Sort phase -----\n",
    "    sorted_set = []\n",
    "    for s in subsets:\n",
    "        # call the serial_sorting method above\n",
    "        sorted_set.append(*pool.apply_async(serial_sorting, [s, buffer_size]).get())\n",
    "    pool.close()\n",
    "    \n",
    "    # ---- Final merge phase ----\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        pool = mp.Pool(processes = N//2)\n",
    "\n",
    "        while True:\n",
    "            if ((N - start_pos) > 2): \n",
    "                subset = dataset[start_pos:start_pos + 2]\n",
    "                merged_set.append(pool.apply(k_way_merge, [subset]))\n",
    "                start_pos += 2\n",
    "            else:\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(pool.apply(k_way_merge, [subset]))\n",
    "                break\n",
    "        \n",
    "        pool.close()\n",
    "        dataset = merged_set\n",
    "        \n",
    "        if (len(dataset) == 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break\n",
    "     \n",
    "  \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMu19MwXLxNd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:[[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_binary_merge_sorting(R, 10, 20)\n",
    "print(\"Final Result:\" + str(result))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5148 -Take_Home_Test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
